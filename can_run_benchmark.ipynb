{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0zLdMswKcLn1"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_DIR = f'/share/u/can/ravel'\n",
    "SRC_DIR = os.path.join(REPO_DIR, 'src')\n",
    "MODEL_DIR = os.path.join(REPO_DIR, 'models')\n",
    "DATA_DIR = os.path.join(REPO_DIR, 'data')\n",
    "\n",
    "for d in [MODEL_DIR, DATA_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "import sys\n",
    "for d in [REPO_DIR, SRC_DIR]:\n",
    "    sys.path.append(d)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyvene as pv\n",
    "\n",
    "import accelerate\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9-AQQvZdOJV"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "751b10379fa04369a928de0c169bc8a5",
      "ee54d0bb220a4fc990300851def006b9",
      "17fa097292c14f368b378455bdb34f00",
      "d8fd7c065e0f40a6a072b9674e151a9b",
      "d7e1ff0cf538459c929ab11f9a085f36",
      "4d0a842f1abd429ea7760a616e543dec",
      "cebd2e681d3348c9854cf39f54cf630c",
      "81a8f701fbf94221bd43b193154cb97e",
      "585aaa1f9d0644e49c672924c4972724",
      "5b620de26aa241a2a1a0013f1e18780f",
      "f7d2396c43894a0c8599dd2c7537d993",
      "70c1b6ad24154b23a2a093b30e90e381",
      "405ac3d1406746e294b95c7bb439da4c",
      "74b005f710cd4e00a3c95ddd67adafce",
      "912af663642347adb8aad11f8a64d549",
      "932170e034ea4cfab7f1441ea918e1d1",
      "b2fda2b2096d40baa193dd4d1a19d87a",
      "52831983547d495bb6fe49ebb529651f",
      "121a8a8060004ec78528b4a6610d258c",
      "299491b58aec4a9a833d5eebb663f7dc",
      "759bd9c482e443f6bbe37061ade5b90e",
      "7bd0541c2f554cf8a928478e91155a2e",
      "810760e38dfd4a93bd3d8d6f403fd8ca",
      "8a44a29537574aa4a646ee4ec2576349",
      "09da734f5e504bca8a1acadabcb5587e",
      "5575f534c7824383a7605a6410a3179b",
      "fed41c03de5b46ef8823018c9b073fb8",
      "14899a8d47f344eb891c7c9a6c8be1f7",
      "cd57e162f7b74446a0513cf506033efe",
      "643f4b03189244ec9ee82058544c6466",
      "1c02c5bc227e434a9cfd61dedbaaa8f6",
      "a128a521389241b0a652ce881c82ffbf",
      "9650c36433ad4834ac0c2e837bdceee1",
      "f37fc22440e84f7daa780c6a2e1aa4ed",
      "ef05646e03e44bbcae8897070c37f715",
      "ddb0fa3f8b7d49f883111688bff55c23",
      "c261774d123c4fd48d410f32c6eebc56",
      "e4c2aba9c08b4fe79a1ba09c3737ddd5",
      "4d30dbf65c7644e6b54de02622f73c78",
      "161946e826514151be288093133e4687",
      "3522ec77b10549519492b2460ea0c067",
      "749a258238d74c8db1c11e835248af30",
      "8b84a2e90dd44a999dc88314222a1202",
      "911948b078154c6a8188760b21782e88",
      "c5ab60d821bd49aaad49220515a978d9",
      "4c429b9b4958486199caae8968f653e1",
      "ac7d96ccf7044106a71e2b78f1203adf",
      "db60dda6866c450dab11f3ecc59c220a",
      "8d40065e7a3b43acaa5d21d8d758e933",
      "eb4cfccc2087486da19ca406e49b3e36",
      "edd02c77ab6e4ccba8306ee65ba90bfb",
      "fd1effd5b7064aca88acd7964d42f980",
      "4a81929a53764f91a7f817dd373bd626",
      "82c060949fbf4cb6932969be027994ea",
      "6c75fbb2a7f743e4bbbb3cd8438e9924",
      "dfe3c2d31f404ff3b3327c17e7c0e380",
      "93bf1b72097248dfa0b3c25a3047447a",
      "245645b8c8f548e2ab70d069fac4c8e2",
      "1a5142f6a25e4b919bf6ecbf77ecd16d",
      "4c333e5bda86444c8cb3bf5f0ab0fd0c",
      "bfc5b7b37a2e4624ad2936f4d33030cd",
      "189ade82b95c4398b8b0edae04294ae5",
      "d2f05dcf69ef401c82199b6492d6e804",
      "02250c1f0df44fe181c36e1a910fde42",
      "6b06ca6b71f34b4eb9f5545dba30d34e",
      "bbaf8442e0524728b4e10edf905985e3",
      "0633455563354788ae5efdd3ee49e48b",
      "49184e4498e8438496db8cb2b33d3cbf",
      "78f4a73ff33f4dee93791bfafd7b0146",
      "ab7a96d0a5f249fe86b64df0e42d0c55",
      "5be2896cb3cc482eb6ab1a0c506f00a6",
      "1c422a05945b44c6a1b12b4a03b46800",
      "b5c9054f8c644877a133d5448216df41",
      "24fe9cd28993448eac55ec8c66cbf0b5",
      "6f5170b160e04e179da978bfdf9dad64",
      "21368d4272824b3e9387701b8c325e0c",
      "6456e65205b749a89f9e721bf145a7ba"
     ]
    },
    "id": "IrrLMNHoqAiF",
    "outputId": "dc3ae720-7a6f-4cfc-affb-a13877e7e85d"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig, LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "# model_name = \"tinyllama\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=MODEL_DIR)\n",
    "# hf_model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_id, low_cpu_mem_usage=True, device_map='auto', cache_dir=MODEL_DIR,\n",
    "#     torch_dtype=torch.bfloat16)\n",
    "# hf_model = hf_model.eval()\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'left'\n",
    "\n",
    "# VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)\n",
    "\n",
    "# layer_idx = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70676e4b25df477ebfba164b722475d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "with open('/share/u/can/src/hf.txt', 'r') as f:\n",
    "    hf_token = f.read().strip()\n",
    "\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "model_name = \"gemma-2-2b\"\n",
    "\n",
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=MODEL_DIR,\n",
    "    token=hf_token,\n",
    "    device_map=device,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=MODEL_DIR,\n",
    "    token=hf_token,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)\n",
    "\n",
    "layer_idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight\n",
    "nnsight_model = NNsight(hf_model)\n",
    "nnsight_tracer_kwargs = {'scan': True, 'validate': False, 'use_cache': False, 'output_attentions': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkttf-519P79"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = 'city'\n",
    "INPUT_MAX_LEN = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jPOMM9W0-BL",
    "outputId": "da9e40e7-3813-4a8e-80f2-ac7d046b8e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training examples=61386, #Validation examples=17528, #Test examples=19255\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "FEATURE_TYPES = datasets.Features({\"input\": datasets.Value(\"string\"), \"label\": datasets.Value(\"string\"),\n",
    "                              \"source_input\": datasets.Value(\"string\"), \"source_label\": datasets.Value(\"string\"),\n",
    "                              \"inv_label\": datasets.Value(\"string\"),\n",
    "                              'split': datasets.Value(\"string\"), 'source_split': datasets.Value(\"string\"),\n",
    "                              'entity': datasets.Value(\"string\"), 'source_entity': datasets.Value(\"string\")})\n",
    "\n",
    "\n",
    "# Load training dataset.\n",
    "split_to_raw_example = json.load(open(os.path.join(DATA_DIR, f'{model_name}/{model_name}_{entity_type}_train.json'), 'r'))\n",
    "# Load validation + test dataset.\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{model_name}/{model_name}_{entity_type}_context_test.json'), 'r')))\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{model_name}/{model_name}_{entity_type}_entity_test.json'), 'r')))\n",
    "# Prepend an extra token to avoid tokenization changes for Llama tokenizer.\n",
    "# Each sequence will start with <s> _ 0\n",
    "SOS_PAD = '0'\n",
    "NUM_SOS_TOKENS = 3\n",
    "for split in split_to_raw_example:\n",
    "  for i in range(len(split_to_raw_example[split])):\n",
    "    split_to_raw_example[split][i]['inv_label'] = SOS_PAD + split_to_raw_example[split][i]['inv_label']\n",
    "    split_to_raw_example[split][i]['label'] = SOS_PAD + split_to_raw_example[split][i]['label']\n",
    "\n",
    "\n",
    "# Load attributes (tasks) to prompt mapping.\n",
    "ALL_ATTR_TO_PROMPTS = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "# Load prompt to intervention location mapping.\n",
    "split_to_entity_pos = json.load(open(os.path.join(DATA_DIR, model_name, f'{model_name}_{entity_type}_prompt_to_entity_position.json')))\n",
    "SPLIT_TO_INV_LOCATIONS = {\n",
    "    f'{task}{split}': {'max_input_length': INPUT_MAX_LEN,\n",
    "                       'inv_position': [INPUT_MAX_LEN + pos]}\n",
    "    for task, pos in split_to_entity_pos.items()\n",
    "    for split in ('-train', '-test', '-val', '')\n",
    "}\n",
    "assert(min([min(v['inv_position']) for v in SPLIT_TO_INV_LOCATIONS.values()]) > 0)\n",
    "\n",
    "\n",
    "# Preprocess the dataset.\n",
    "def filter_inv_example(example):\n",
    "  return (example['label'] != example['inv_label'] and\n",
    "          example['source_split'] in SPLIT_TO_INV_LOCATIONS and\n",
    "          example['split'] in SPLIT_TO_INV_LOCATIONS)\n",
    "\n",
    "for split in split_to_raw_example:\n",
    "  random.shuffle(split_to_raw_example[split])\n",
    "  split_to_raw_example[split] = list(filter(filter_inv_example, split_to_raw_example[split]))\n",
    "  if len(split_to_raw_example[split]) == 0:\n",
    "    print('Empty split: \"%s\"' % split)\n",
    "# Remove empty splits.\n",
    "split_to_raw_example = {k: v for k, v in split_to_raw_example.items() if len(v) > 0}\n",
    "print(f\"#Training examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-train')]))}, \"\n",
    "      f\"#Validation examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-val')]))}, \"\n",
    "      f\"#Test examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-test')]))}\")\n",
    "split_to_dataset = {split: Dataset.from_list(\n",
    "    split_to_raw_example[split], features=FEATURE_TYPES)\n",
    "                    for split in split_to_raw_example}\n",
    "\n",
    "# #Training examples=116728, #Validation examples=20516, #Test examples=22497"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKpdF24zzJN-"
   },
   "source": [
    "# Sparse Autoencoder (SAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umZuIUk2DZaO"
   },
   "source": [
    "## Tinyllama SAE Training\n",
    "\n",
    "We will train a sparse autoencoder on entity representations extracted offline.\n",
    "\n",
    "* Download entity representations extracted from the Wikipedia dataset [here](https://drive.google.com/file/d/1hZ-Nv3ehf0Ok4ic3ybe-DATEh-HRjYkt/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzRVRB6DDZYe",
    "outputId": "80197f5d-6504-4551-c2e7-d2004bffdaa8"
   },
   "outputs": [],
   "source": [
    "# from scripts.train_sae import train_sae\n",
    "# import re\n",
    "\n",
    "# config = {\n",
    "#     'task_name': task_name,\n",
    "#     'reg_coeff': float(re.search('reg([\\d.]+)', task_name).group(1)),\n",
    "#     'input_dim': model.config.hidden_size,\n",
    "#     'latent_dim': int(re.search('dim(\\d+)', task_name).group(1)),\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'weight_decay': 1e-4,\n",
    "#     'end_learning_rate_ratio': 0.5,\n",
    "#     'num_epochs': int(re.search('ep(\\d+)', task_name).group(1)),\n",
    "#     'model_dir': MODEL_DIR,\n",
    "#     'log_dir': os.path.join(MODEL_DIR, 'logs', task_name),\n",
    "# }\n",
    "\n",
    "# # Training metrics are logged to the Tensorboard at http://localhost:6006/.\n",
    "# # autoencoder = train_sae(config, wiki_train_dataloader, wiki_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mf6E0eudgO2F"
   },
   "outputs": [],
   "source": [
    "# autoencoder_run_name = 'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k.pt'\n",
    "# autoencoder = torch.load(os.path.join(MODEL_DIR, autoencoder_run_name)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma2 JumpRelu SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_repo_id = \"google/gemma-scope-2b-pt-res\"\n",
    "# sae_filename = \"layer_20/width_16k/average_l0_71/params.npz\"\n",
    "sae_filename = \"layer_14/width_16k/average_l0_43/params.npz\"\n",
    "sae_filename = \"layer_10/width_16k/average_l0_77/params.npz\"\n",
    "\n",
    "layer_idx = 10\n",
    "\n",
    "autoencoder_run_name = (sae_repo_id + '-' + sae_filename.replace('/', '-')).replace('.npz', '')\n",
    "\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=sae_repo_id,\n",
    "    filename=sae_filename,\n",
    "    force_download=False,\n",
    "    cache_dir= os.path.join(MODEL_DIR, model_name),\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}\n",
    "\n",
    "\n",
    "\n",
    "class JumpReluAutoEncoder(torch.nn.Module):\n",
    "  \"\"\"Sparse Autoencoder with a two-layer encoder and a two-layer decoder.\"\"\"\n",
    "\n",
    "  def __init__(self, embed_dim, latent_dim, device):\n",
    "    super().__init__()\n",
    "    self.dtype = torch.float32\n",
    "    self.embed_dim = embed_dim\n",
    "    self.latent_dim = latent_dim\n",
    "    self.W_enc = nn.Parameter(torch.empty(embed_dim, latent_dim))\n",
    "    self.b_enc = nn.Parameter(torch.zeros(latent_dim))\n",
    "    self.W_dec = nn.Parameter(torch.empty(latent_dim, embed_dim))\n",
    "    self.b_dec = nn.Parameter(torch.zeros(embed_dim))\n",
    "    self.threshold = nn.Parameter(torch.zeros(latent_dim))\n",
    "    self.autoencoder_losses = {}\n",
    "\n",
    "  def encode(self, x, normalize_input=False):\n",
    "    if normalize_input:\n",
    "      raise ValueError(\"Not supported\")\n",
    "      x = x - self.decoder[0].bias\n",
    "    pre_jump = x @ self.W_enc + self.b_enc\n",
    "\n",
    "    f = nn.ReLU()(pre_jump * (pre_jump > self.threshold))\n",
    "    # Decoder weights are not normalized. Thus we have to compensate here to get comparabe feature activations.\n",
    "    f = f * self.W_dec.norm(dim=1)\n",
    "    return f\n",
    "\n",
    "  def decode(self, z):\n",
    "    # Decoder weights are not normalized. Thus we have to compensate here to get comparabe feature activations.\n",
    "    z = z / self.W_dec.norm(dim=1)\n",
    "    return z @ self.W_dec + self.b_dec\n",
    "\n",
    "  def forward(self, base):\n",
    "    base_type = base.dtype\n",
    "    base = base.to(self.dtype)\n",
    "    self.autoencoder_losses.clear()\n",
    "    z = self.encode(base)\n",
    "    base_reconstruct = self.decode(z)\n",
    "    # The sparsity objective.\n",
    "    l1_loss = torch.nn.functional.l1_loss(z, torch.zeros_like(z))\n",
    "    # The reconstruction objective.\n",
    "    l2_loss = torch.mean((base_reconstruct - base)**2)\n",
    "    self.autoencoder_losses['l1_loss'] = l1_loss\n",
    "    self.autoencoder_losses['l2_loss'] = l2_loss\n",
    "    return {'latent': z, 'output': base_reconstruct.to(base_type)}\n",
    "\n",
    "  def get_autoencoder_losses(self):\n",
    "    return self.autoencoder_losses\n",
    "\n",
    "  def from_pretrained(\n",
    "          path: str | None = None, \n",
    "          load_from_sae_lens: bool = False,\n",
    "          device: torch.device | None = None,\n",
    "          **kwargs,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Load a pretrained autoencoder from a file.\n",
    "    If sae_lens=True, then pass **kwargs to sae_lens's\n",
    "    loading function.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(path)\n",
    "    latent_dim, embed_dim = state_dict['W_enc'].shape\n",
    "    autoencoder = JumpReluAutoEncoder(embed_dim, latent_dim)\n",
    "    autoencoder.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "embed_dim = params['W_enc'].shape[0]\n",
    "latent_dim = params['W_enc'].shape[1]\n",
    "\n",
    "sae = JumpReluAutoEncoder(\n",
    "    embed_dim=embed_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    device=device,\n",
    ")\n",
    "sae.load_state_dict(pt_params)\n",
    "sae.to(device)\n",
    "\n",
    "autoencoder = sae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBClAbyY0Tm0"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp data/tinyllama/ravel_city_tinyllama_layer14_representation.hdf5 data/ravel_city_tinyllama_layer14_representation.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OeMovewbVdp",
    "outputId": "f4897d1b-43b3-4fe4-c103-0ea1e57b1296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unique labels=5 ['Africa', 'Asia', 'Europe', 'North America', 'South America']\n",
      "#unique labels=55 ['Albania', 'Angola', 'Bangladesh', 'Brazil', 'Bulgaria', 'Canada', 'Chile', 'China', 'Colombia', 'Costa Rica', 'Croatia', 'Denmark', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Gabon', 'Germany', 'Greece', 'Hungary', 'India', 'Indonesia', 'Iran', 'Iraq', 'Italy', 'Japan', 'Latvia', 'Lithuania', 'Mexico', 'Montenegro', 'Mozambique', 'Myanmar', 'Nepal', 'Nicaragua', 'Nigeria', 'Norway', 'Poland', 'Portugal', 'Romania', 'Russia', 'Serbia', 'Sierra Leone', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Tunisia', 'Turkey', 'Uganda', 'Ukraine', 'Venezuela']\n",
      "#unique labels=39 ['Albanian', 'Arabic', 'Bengali', 'Bulgarian', 'Burmese', 'Chinese', 'Croatian', 'Danish', 'English', 'Estonian', 'Finnish', 'French', 'German', 'Greek', 'Hausa', 'Hindi', 'Hungarian', 'Indonesian', 'Italian', 'Japanese', 'Korean', 'Latvian', 'Lithuanian', 'Montenegrin', 'Nepali', 'Norwegian', 'Persian', 'Polish', 'Portuguese', 'Romanian', 'Russian', 'Serbian', 'Sinhala', 'Spanish', 'Swedish', 'Thai', 'Turkish', 'Ukrainian', 'Yoruba']\n",
      "#unique labels=75 ['-1', '-12', '-23', '-25', '-26', '-27', '-29', '-3', '-34', '-6', '-7', '0', '1', '10', '11', '12', '13', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '26', '27', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '68', '69', '7', '70', '8', '9']\n",
      "#unique labels=105 ['-1', '-100', '-101', '-102', '-120', '-124', '-13', '-2', '-3', '-49', '-51', '-6', '-60', '-68', '-71', '-75', '-76', '-77', '-79', '-8', '-84', '-86', '-9', '-99', '0', '1', '10', '100', '101', '102', '106', '107', '11', '110', '113', '12', '121', '122', '127', '129', '13', '130', '131', '132', '134', '135', '136', '137', '138', '139', '14', '140', '141', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '36', '37', '38', '39', '4', '40', '41', '44', '45', '48', '49', '5', '50', '51', '52', '6', '7', '77', '78', '8', '80', '81', '85', '89', '9', '90', '91', '95', '96', '99']\n",
      "#unique labels=58 ['Africa/Cairo', 'Africa/Freetown', 'Africa/Kampala', 'Africa/Lagos', 'Africa/Libreville', 'Africa/Lome', 'Africa/Luanda', 'Africa/Maputo', 'Africa/Tunis', 'America/Bogota', 'America/Caracas', 'America/Costa_Rica', 'America/Guayaquil', 'America/Managua', 'America/Manaus', 'America/Mexico_City', 'America/Monterrey', 'America/Santiago', 'America/Sao_Paulo', 'America/Vancouver', 'Asia/Baghdad', 'Asia/Bangkok', 'Asia/Colombo', 'Asia/Dhaka', 'Asia/Jakarta', 'Asia/Kathmandu', 'Asia/Kolkata', 'Asia/Rangoon', 'Asia/Seoul', 'Asia/Shanghai', 'Asia/Tehran', 'Asia/Tokyo', 'Europe/Athens', 'Europe/Belgrade', 'Europe/Berlin', 'Europe/Bucharest', 'Europe/Budapest', 'Europe/Copenhagen', 'Europe/Helsinki', 'Europe/Istanbul', 'Europe/Kiev', 'Europe/Lisbon', 'Europe/Madrid', 'Europe/Moscow', 'Europe/Oslo', 'Europe/Paris', 'Europe/Podgorica', 'Europe/Riga', 'Europe/Rome', 'Europe/Simferopol', 'Europe/Sofia', 'Europe/Stockholm', 'Europe/Tallinn', 'Europe/Tirane', 'Europe/Vilnius', 'Europe/Warsaw', 'Europe/Zagreb', 'Europe/Zurich']\n"
     ]
    }
   ],
   "source": [
    "# Load the RAVEL dataset.\n",
    "import json\n",
    "\n",
    "from src.utils.dataset_utils import load_entity_representation_with_label\n",
    "\n",
    "splits = ['train', 'val_entity', 'val_context']\n",
    "feature_hdf5_path = os.path.join(DATA_DIR, model_name, f'ravel_{entity_type}_{model_name}_layer{layer_idx}_representation.hdf5')\n",
    "entity_attr_to_label = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_entity_attributes.json')))\n",
    "X, Y, sorted_unique_label = load_entity_representation_with_label(feature_hdf5_path, entity_attr_to_label, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sPF6FR8zvwvJ"
   },
   "outputs": [],
   "source": [
    "# Run feature selection.\n",
    "import numpy as np\n",
    "\n",
    "from src.methods.select_features import select_features_with_classifier\n",
    "\n",
    "intervention_dim_to_eval = [\n",
    "    ('reconstruction', None),\n",
    "    ('dim%d' % autoencoder.latent_dim, range(autoencoder.latent_dim))]\n",
    "\n",
    "attr = 'Country'\n",
    "coeff_to_kept_dims = select_features_with_classifier(\n",
    "    autoencoder.encode, torch.from_numpy(X[attr]['train']).to(device), Y[attr]['train'])\n",
    "for kept_dim in coeff_to_kept_dims.values():\n",
    "  intervention_dim_to_eval.append(('dim%d' % len(kept_dim), kept_dim))\n",
    "# Random baselines.\n",
    "for i in [64, 512]:\n",
    "    kept_dim = np.random.permutation(autoencoder.latent_dim)[:i]\n",
    "    intervention_dim_to_eval.append(('random_dim%d' % len(kept_dim), kept_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O tinyllama.tgz \"https://huggingface.co/datasets/adamkarvonen/ravel/resolve/main/tinyllama.tgz?download=true\"\n",
    "# !tar -xzf tinyllama.tgz -C data/\n",
    "# !mkdir data/base\n",
    "# !tar -xvf data.tgz -C data/base --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0npE3xlExIz"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERY IMPORTANT NOTE\n",
    "In the below cell, we only use the first 10 elements of the dataset to speed up iteration. This should get increased once we have everything working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "# Run eval\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test') or k.endswith('-val')\n",
    "                         }\n",
    "print(len(eval_split_to_dataset))\n",
    "\n",
    "# # Keep only the first 10 items\n",
    "# eval_split_to_dataset = dict(list(eval_split_to_dataset.items())[:10])\n",
    "# print(f\"New length: {len(eval_split_to_dataset)}\")\n",
    "\n",
    "# print(len(eval_split_to_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default pyvene implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyvene_intervention(\n",
    "    intervenable,\n",
    "    split_to_inv_locations,\n",
    "    inputs,\n",
    "    b_s,\n",
    "    num_inv,\n",
    "    max_new_tokens,\n",
    "    intervention_locations,\n",
    "    forward_only=False,\n",
    "):\n",
    "    if not forward_only:\n",
    "        base_outputs, counterfactual_out_tokens = intervenable.generate(\n",
    "            {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            },\n",
    "            [\n",
    "                {\n",
    "                    \"input_ids\": inputs[\"source_input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"source_attention_mask\"],\n",
    "                    \"position_ids\": inputs[\"source_position_ids\"],\n",
    "                }\n",
    "            ],\n",
    "            intervention_locations,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            intervene_on_prompt=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            output_original_output=True,\n",
    "        )\n",
    "    # else: # This seems deprecated in the demo notebook\n",
    "    #     base_outputs, counterfactual_outputs = intervenable(\n",
    "    #         {\n",
    "    #             \"input_ids\": inputs[\"input_ids\"],\n",
    "    #             \"attention_mask\": inputs[\"attention_mask\"],\n",
    "    #             \"position_ids\": inputs[\"position_ids\"],\n",
    "    #         },\n",
    "    #         [\n",
    "    #             {\n",
    "    #                 \"input_ids\": inputs[\"source_input_ids\"],\n",
    "    #                 \"attention_mask\": inputs[\"source_attention_mask\"],\n",
    "    #                 \"position_ids\": inputs[\"source_position_ids\"],\n",
    "    #             }\n",
    "    #         ],\n",
    "    #         intervention_locations,\n",
    "    #         output_original_output=True,\n",
    "    #     )\n",
    "    #     counterfactual_logits = counterfactual_outputs.logits\n",
    "    #     counterfactual_out_tokens = torch.argmax(counterfactual_outputs.logits, dim=-1)\n",
    "    #     base_outputs = torch.argmax(base_outputs.logits, dim=-1)\n",
    "\n",
    "    return base_outputs, counterfactual_out_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nnsight replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1 base_token decoded: Paris\n",
      "prompt1 source_token decoded: Tokyo\n",
      "prompt2 base_token decoded:  London\n",
      "prompt2 source_token decoded: Berlin\n",
      "counterfactual_out_tokens decoded: ['Paris is in the country of Japan', 'The main language spoken in the city of London is German']\n",
      "counterfactual_out_tokens decoded: ['Paris is in the country of Japan.', 'The main language spoken in the city of London is German.']\n",
      "counterfactual_out_tokens decoded: ['Paris is in the country of Japan.\\n\\n', 'The main language spoken in the city of London is German. The']\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "def nnsight_intervention(nnsight_model, layer, autoencoder, inv_dims, inputs, split_to_inv_locations, n_generated_tokens, device, add_reconstruction_error=True, inv_positions=None, verbose=False):\n",
    "    batch_size = inputs['input_ids'].shape[0]\n",
    "    submodule = nnsight_model.model.layers[layer]\n",
    "    \n",
    "    # Organize intervention positions\n",
    "    if inv_positions is None: # Default, custom intervention positions only for testing\n",
    "        base_inv_positions = torch.tensor([split_to_inv_locations[inputs[\"split\"][i]][\"inv_position\"] for i in range(batch_size)], device=device)\n",
    "        source_inv_positions = torch.tensor([split_to_inv_locations[inputs[\"source_split\"][i]][\"inv_position\"] for i in range(batch_size)], device=device)\n",
    "    else:\n",
    "        base_inv_positions, source_inv_positions = inv_positions\n",
    "    \n",
    "    # Indexing preparation\n",
    "    if isinstance(inv_dims, range):\n",
    "        inv_dims = torch.tensor(list(inv_dims), device=device, dtype=torch.int)\n",
    "    if len(base_inv_positions.shape) > 1:\n",
    "        base_inv_positions = base_inv_positions.squeeze(dim=-1)\n",
    "    if len(source_inv_positions.shape) > 1:\n",
    "        source_inv_positions = source_inv_positions.squeeze(dim=-1)\n",
    "\n",
    "    batch_arange = einops.repeat(torch.arange(batch_size, device=device, dtype=torch.int), 'b -> b d', d=inv_dims.shape[0])\n",
    "    base_inv_positions = einops.repeat(base_inv_positions, 'b -> b d', d=inv_dims.shape[0])\n",
    "    source_inv_positions = einops.repeat(source_inv_positions, 'b -> b d', d=inv_dims.shape[0])\n",
    "    inv_dims = einops.repeat(inv_dims, 'd -> b d', b=batch_size)\n",
    "\n",
    "    # Forward pass on source input\n",
    "    with torch.no_grad(), nnsight_model.trace(inputs['source_input_ids'], attention_mask=inputs['source_attention_mask'], **nnsight_tracer_kwargs):\n",
    "        source_sae_acts = autoencoder.encode(submodule.output[0])\n",
    "        source_sae_acts = source_sae_acts[batch_arange, source_inv_positions, inv_dims].save()\n",
    "\n",
    "    # Forward pass on base input with intervention\n",
    "    generated_inputs_shape = inputs['input_ids'].shape[:-1] + (inputs['input_ids'].shape[-1] + n_generated_tokens,)\n",
    "    generated_inputs = torch.zeros(generated_inputs_shape, device=device, dtype=inputs['input_ids'].dtype)\n",
    "    generated_inputs[:, :inputs['input_ids'].shape[-1]] = inputs['input_ids']\n",
    "    generated_attn_mask = torch.cat([inputs['attention_mask'], torch.ones(batch_size, n_generated_tokens, device=device)], dim=-1)\n",
    "    n_tokens = generated_attn_mask.sum(dim=-1).to(torch.int)\n",
    "    generated_pos_ids = torch.zeros_like(generated_inputs, device=device, dtype=inputs['input_ids'].dtype)\n",
    "    for batch_idx in range(batch_size):\n",
    "        generated_pos_ids[batch_idx, -n_tokens[batch_idx]:] = torch.arange(n_tokens[batch_idx], device=device, dtype=inputs['input_ids'].dtype)\n",
    "\n",
    "    for i in range(n_generated_tokens):\n",
    "        with torch.no_grad(), nnsight_model.trace(generated_inputs, attention_mask=generated_attn_mask, position_ids=generated_pos_ids, **nnsight_tracer_kwargs):\n",
    "            llm_acts = submodule.output[0]\n",
    "            base_sae_acts = autoencoder.encode(llm_acts)\n",
    "            llm_acts_reconstructed = autoencoder.decode(base_sae_acts)\n",
    "\n",
    "            base_sae_acts[batch_arange, base_inv_positions, inv_dims] = source_sae_acts\n",
    "            llm_acts_intervened = autoencoder.decode(base_sae_acts)\n",
    "\n",
    "            if not add_reconstruction_error:\n",
    "                submodule.output = (llm_acts_intervened.to(llm_acts.dtype),)\n",
    "            else:\n",
    "                reconstruction_error = llm_acts - llm_acts_reconstructed\n",
    "                corrected_acts = llm_acts_intervened + reconstruction_error\n",
    "                submodule.output = (corrected_acts.to(llm_acts.dtype),)\n",
    "\n",
    "            counterfactual_logits = nnsight_model.lm_head.output.save()\n",
    "\n",
    "        # Append generation\n",
    "        final_token_pos = i - n_generated_tokens\n",
    "        next_token = torch.argmax(counterfactual_logits[:, final_token_pos-1, :], dim=-1)\n",
    "        generated_inputs[:, final_token_pos] = next_token\n",
    "\n",
    "        if verbose:\n",
    "            print(f'counterfactual_out_tokens decoded: {tokenizer.batch_decode(generated_inputs, skip_special_tokens=True)}')\n",
    "\n",
    "    return generated_inputs\n",
    "\n",
    "\n",
    "\n",
    "# Test the intervention\n",
    "\n",
    "batch_size = 2\n",
    "intervention_dims = range(autoencoder.latent_dim)\n",
    "base_promts  = ['Paris is in the country of', 'The main language spoken in the city of London is']\n",
    "source_prompts = ['Tokyo is big.', 'Berlin is exciting.']\n",
    "n_generated_tokens = 3\n",
    "add_reconstruction_error = True\n",
    "\n",
    "base_tok = tokenizer(base_promts, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=INPUT_MAX_LEN)\n",
    "source_tok = tokenizer(source_prompts, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=INPUT_MAX_LEN)\n",
    "base_tok = hf_model.prepare_inputs_for_generation(**base_tok)\n",
    "source_tok = hf_model.prepare_inputs_for_generation(**source_tok)\n",
    "base_inv_pos = torch.ones(batch_size, device=device, dtype=torch.int) * -1\n",
    "source_inv_pos = torch.ones(batch_size, device=device, dtype=torch.int) * -1\n",
    "base_inv_pos = torch.tensor([[42], [46]], dtype=torch.int)\n",
    "\n",
    "# # Tinyllama\n",
    "# source_inv_pos = torch.tensor([[44], [43]], dtype=torch.int)\n",
    "\n",
    "# Gemma2-2b\n",
    "source_inv_pos = torch.tensor([[44], [44]], dtype=torch.int)\n",
    "\n",
    "# print(f'base_inv_pos: {base_inv_pos}')\n",
    "# print(f'source_inv_pos: {source_inv_pos}')\n",
    "\n",
    "# for tok_ids in source_tok['input_ids']:\n",
    "#     for j, tok in enumerate(tok_ids):\n",
    "#         print(f'{j}, tok: {tokenizer.decode(tok)}')\n",
    "\n",
    "\n",
    "# for i in torch.arange(1, len(base_tok['input_ids'][0]), device=device, dtype=torch.int) * -1:\n",
    "    # if i == 10:\n",
    "    #         break\n",
    "#     print(f'i: {i}')\n",
    "#     base_inv_pos = torch.tensor([[i]])\n",
    "print(f'prompt1 base_token decoded: {tokenizer.decode(base_tok[\"input_ids\"][0][base_inv_pos[0][0]])}')\n",
    "print(f'prompt1 source_token decoded: {tokenizer.decode(source_tok[\"input_ids\"][0][source_inv_pos[0][0]])}')\n",
    "print(f'prompt2 base_token decoded: {tokenizer.decode(base_tok[\"input_ids\"][1][base_inv_pos[1][0]])}')\n",
    "print(f'prompt2 source_token decoded: {tokenizer.decode(source_tok[\"input_ids\"][1][source_inv_pos[1][0]])}')\n",
    "\n",
    "inputs = {\n",
    "    'input_ids': base_tok['input_ids'].to(device),\n",
    "    'attention_mask': base_tok['attention_mask'].to(device),\n",
    "    'position_ids': base_tok['position_ids'].to(device),\n",
    "    'source_input_ids': source_tok['input_ids'].to(device),\n",
    "    'source_attention_mask': source_tok['attention_mask'].to(device),\n",
    "    'source_position_ids': source_tok['position_ids'].to(device),\n",
    "}\n",
    "\n",
    "\n",
    "counterfactual_out_tokens = nnsight_intervention(\n",
    "    nnsight_model,\n",
    "    layer_idx,\n",
    "    autoencoder,\n",
    "    intervention_dims,\n",
    "    inputs,\n",
    "    None, #split_to_inv_locations is not used in the test\n",
    "    n_generated_tokens,\n",
    "    device,\n",
    "    add_reconstruction_error,\n",
    "    inv_positions=(base_inv_pos, source_inv_pos),\n",
    "    verbose=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.intervention_utils import (\n",
    "    is_llama_tokenizer,\n",
    "    get_dataloader,\n",
    "    remove_invalid_token_id,\n",
    "    load_intervenable_with_autoencoder, \n",
    "    remove_all_forward_hooks\n",
    ")\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def eval_with_interventions(\n",
    "    hf_model, # Native Hugging Face model\n",
    "    nnsight_model,  # NNsight model wrapper\n",
    "    split_to_dataset,\n",
    "    split_to_inv_locations,\n",
    "    tokenizer,\n",
    "    inv_dims,\n",
    "    compute_metrics_fn,\n",
    "    max_new_tokens=1,\n",
    "    eval_batch_size=16,\n",
    "    debug_print=False,\n",
    "    forward_only=False,\n",
    "    use_nnsight_replication=False,\n",
    "    device='cuda',\n",
    "):\n",
    "    if not use_nnsight_replication:\n",
    "        intervenable = load_intervenable_with_autoencoder(hf_model, autoencoder, inv_dims, layer_idx)\n",
    "        intervenable.set_device(\"cuda\")\n",
    "        intervenable.disable_model_gradients()\n",
    "        num_inv = len(intervenable.interventions)\n",
    "    else:\n",
    "        num_inv = 1\n",
    "\n",
    "    split_to_eval_metrics = {}\n",
    "    padding_offset = 3 if is_llama_tokenizer(tokenizer) else 0\n",
    "    for split in tqdm(split_to_dataset):\n",
    "        # Asssume all inputs have the same max length.\n",
    "        prompt_max_length = split_to_inv_locations[split_to_dataset[split][0][\"split\"]][\n",
    "            \"max_input_length\"\n",
    "        ]\n",
    "        eval_dataloader = get_dataloader(\n",
    "            split_to_dataset[split],\n",
    "            tokenizer=tokenizer,\n",
    "            batch_size=eval_batch_size,\n",
    "            prompt_max_length=prompt_max_length,\n",
    "            output_max_length=padding_offset + max_new_tokens,\n",
    "            first_n=max_new_tokens,\n",
    "        )\n",
    "        eval_labels = collections.defaultdict(list)\n",
    "        eval_preds = []\n",
    "        with torch.no_grad():\n",
    "            if debug_print:\n",
    "                epoch_iterator = tqdm(eval_dataloader, desc=f\"Test\")\n",
    "            else:\n",
    "                epoch_iterator = eval_dataloader\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                b_s = inputs[\"input_ids\"].shape[0]\n",
    "                position_ids = {\n",
    "                    f\"{prefix}position_ids\": hf_model.prepare_inputs_for_generation(\n",
    "                        input_ids=inputs[f\"{prefix}input_ids\"],\n",
    "                        attention_mask=inputs[f\"{prefix}attention_mask\"],\n",
    "                    )[\"position_ids\"]\n",
    "                    for prefix in (\"\", \"source_\")\n",
    "                }\n",
    "                inputs.update(position_ids)\n",
    "                for key in inputs:\n",
    "                    if key in (\n",
    "                        \"input_ids\",\n",
    "                        \"source_input_ids\",\n",
    "                        \"attention_mask\",\n",
    "                        \"source_attention_mask\",\n",
    "                        \"position_ids\",\n",
    "                        \"source_position_ids\",\n",
    "                        \"labels\",\n",
    "                        \"base_labels\",\n",
    "                    ):\n",
    "                        inputs[key] = inputs[key].to(hf_model.device)\n",
    "\n",
    "                intervention_locations = {\n",
    "                    \"sources->base\": (\n",
    "                        [\n",
    "                            [\n",
    "                                split_to_inv_locations[inputs[\"source_split\"][i]][\"inv_position\"]\n",
    "                                for i in range(b_s)\n",
    "                            ]\n",
    "                        ]\n",
    "                        * num_inv,\n",
    "                        [\n",
    "                            [\n",
    "                                split_to_inv_locations[inputs[\"split\"][i]][\"inv_position\"]\n",
    "                                for i in range(b_s)\n",
    "                            ]\n",
    "                        ]\n",
    "                        * num_inv,\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                if not use_nnsight_replication:\n",
    "                    base_outputs, counterfactual_out_tokens = pyvene_intervention(\n",
    "                        intervenable,\n",
    "                        split_to_inv_locations,\n",
    "                        inputs,\n",
    "                        b_s,\n",
    "                        num_inv,\n",
    "                        max_new_tokens,\n",
    "                        forward_only=forward_only,\n",
    "                    )\n",
    "                    eval_preds.append(counterfactual_out_tokens)\n",
    "                else:\n",
    "                    base_outputs = hf_model.generate(\n",
    "                        inputs[\"input_ids\"],\n",
    "                        attention_mask=inputs[\"attention_mask\"],\n",
    "                        max_length=inputs[\"input_ids\"].shape[1] + max_new_tokens,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        num_beams=1,\n",
    "                        do_sample=False,\n",
    "                        output_scores=True,\n",
    "                    )\n",
    "                    counterfactual_out_tokens = nnsight_intervention(\n",
    "                        nnsight_model,\n",
    "                        layer_idx,\n",
    "                        autoencoder,\n",
    "                        inv_dims,\n",
    "                        inputs,\n",
    "                        split_to_inv_locations,\n",
    "                        n_generated_tokens=max_new_tokens,\n",
    "                        device=device,\n",
    "                        add_reconstruction_error=True,\n",
    "                    )\n",
    "                    eval_preds.append(counterfactual_out_tokens)\n",
    "                    \n",
    "                for label_type in [\"base_labels\", \"labels\"]:\n",
    "                    eval_labels[label_type].append(inputs[label_type])\n",
    "                eval_labels[\"base_outputs\"].append(base_outputs[:, -max_new_tokens:])\n",
    "\n",
    "                if debug_print and step < 3:\n",
    "                    print(\"\\nInputs:\")\n",
    "                    print(\"Base:\", inputs[\"input\"][:3])\n",
    "                    print(\"Source:\", inputs[\"source_input\"][:3])\n",
    "                    print(\"Tokens to intervene:\")\n",
    "                    print(\n",
    "                        \"Base:\",\n",
    "                        tokenizer.batch_decode(\n",
    "                            [\n",
    "                                inputs[\"input_ids\"][i][\n",
    "                                    intervention_locations[\"sources->base\"][1][0][i]\n",
    "                                ]\n",
    "                                for i in range(len(inputs[\"split\"]))\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Source:\",\n",
    "                        tokenizer.batch_decode(\n",
    "                            [\n",
    "                                inputs[\"source_input_ids\"][i][\n",
    "                                    intervention_locations[\"sources->base\"][0][0][i]\n",
    "                                ]\n",
    "                                for i in range(len(inputs[\"split\"]))\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                    base_output_text = tokenizer.batch_decode(\n",
    "                        base_outputs[:, -max_new_tokens:], skip_special_tokens=True\n",
    "                    )\n",
    "                    print(\"Base Output:\", base_output_text)\n",
    "                    print(\n",
    "                        \"Output:    \",\n",
    "                        tokenizer.batch_decode(counterfactual_out_tokens[:, -max_new_tokens:]),\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Inv Label: \",\n",
    "                        tokenizer.batch_decode(\n",
    "                            remove_invalid_token_id(\n",
    "                                inputs[\"labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                            )\n",
    "                        ),\n",
    "                    )\n",
    "                    base_label_text = tokenizer.batch_decode(\n",
    "                        remove_invalid_token_id(\n",
    "                            inputs[\"base_labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                        ),\n",
    "                        skip_special_tokens=True,\n",
    "                    )\n",
    "                    print(\"Base Label:\", base_label_text)\n",
    "                    if base_label_text != base_output_text:\n",
    "                        print(\"WARNING: Base outputs does not match base labels!\")\n",
    "                        \n",
    "        eval_metrics = {\n",
    "            label_type: compute_metrics_fn(\n",
    "                tokenizer,\n",
    "                eval_preds,\n",
    "                eval_labels[label_type],\n",
    "                last_n_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                extra_labels=eval_labels,\n",
    "                eval_label_type=label_type,\n",
    "            )\n",
    "            for label_type in eval_labels\n",
    "            if label_type.endswith(\"labels\")\n",
    "        }\n",
    "        print(\"\\n\", repr(split) + \":\", eval_metrics)\n",
    "        split_to_eval_metrics[split] = {\n",
    "            \"metrics\": eval_metrics,\n",
    "            \"inv_outputs\": tokenizer.batch_decode(counterfactual_out_tokens[:, -max_new_tokens:]),\n",
    "            \"inv_labels\": tokenizer.batch_decode(\n",
    "                remove_invalid_token_id(\n",
    "                    inputs[\"labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                )\n",
    "            ),\n",
    "            \"base_labels\": tokenizer.batch_decode(\n",
    "                remove_invalid_token_id(\n",
    "                    inputs[\"base_labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    if not use_nnsight_replication:\n",
    "        remove_all_forward_hooks(intervenable)\n",
    "        del intervenable\n",
    "    return split_to_eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(tokenizer, eval_preds, eval_labels, pad_token_id, last_n_tokens=1, **kwargs):\n",
    "    \"\"\"Computes squence-level and token-level accuracy.\"\"\"\n",
    "    total_count, total_token_count = 0, 0\n",
    "    correct_count, correct_token_count = 0, 0\n",
    "    for eval_pred, eval_label in zip(eval_preds, eval_labels):\n",
    "        actual_test_labels = eval_label[:, -last_n_tokens:]\n",
    "        if len(eval_pred.shape) == 3:\n",
    "            # eval_preds is in the form of logits.\n",
    "            pred_test_labels = torch.argmax(eval_pred[:, -last_n_tokens:], dim=-1)\n",
    "        else:\n",
    "            # eval_preds is in the form of token ids.\n",
    "            pred_test_labels = eval_pred[:, -last_n_tokens:]\n",
    "        padding_tokens = torch.logical_or(\n",
    "            actual_test_labels == pad_token_id, actual_test_labels < 0\n",
    "        )\n",
    "        match_tokens = actual_test_labels == pred_test_labels\n",
    "        correct_labels = torch.logical_or(match_tokens, padding_tokens)\n",
    "        total_count += len(correct_labels)\n",
    "        correct_count += torch.all(correct_labels, axis=-1).float().sum().tolist()\n",
    "        total_token_count += (~padding_tokens).float().sum().tolist()\n",
    "        correct_token_count += (~padding_tokens & match_tokens).float().sum().tolist()\n",
    "    accuracy = round(correct_count / total_count, 2)\n",
    "    token_accuracy = round(correct_token_count / total_token_count, 2)\n",
    "    return {\"accuracy\": accuracy, \"token_accuracy\": token_accuracy}\n",
    "\n",
    "def compute_metrics_string_matching(tokenizer, eval_preds, eval_labels, last_n_tokens, **kwargs):\n",
    "    \"\"\"Computes squence-level and string-level accuracy.\"\"\"\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    eval_preds = torch.cat(eval_preds, dim=0)\n",
    "    eval_labels = torch.cat(eval_labels, dim=0)[:, -last_n_tokens:]\n",
    "    eval_preds_str = tokenizer.batch_decode(eval_preds, skip_special_tokens=True)\n",
    "    eval_labels_str = tokenizer.batch_decode(eval_labels, skip_special_tokens=True)\n",
    "    eval_labels_str = [l[1:].strip() for l in eval_labels_str]\n",
    "\n",
    "    for p, l in zip(eval_preds_str, eval_labels_str):\n",
    "        # print(f'p: {p}, l: {l}')\n",
    "        total_count += 1\n",
    "        if l.lower() in p.lower():\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = round(correct_count / total_count, 2)\n",
    "    return {\"accuracy\": accuracy}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer=10\n",
      "Intervention_dims=range(0, 16384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/112 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99feab8cd7544ddf9f9ebe3c05bc9c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a55ec28675449ad8baf2d52acebabfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/112 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntervention_dims=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minv_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m split_to_eval_metrics \u001b[38;5;241m=\u001b[39m \u001b[43meval_with_interventions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnnsight_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnsight_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43msplit_to_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_split_to_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43msplit_to_inv_locations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPLIT_TO_INV_LOCATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43minv_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcompute_metrics_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics_string_matching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m  \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 21GB RAM usage with RTX A6000\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m  \u001b[49m\u001b[43muse_nnsight_replication\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nnsight_replication\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(split_to_eval_metrics, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautoencoder_run_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtok_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_task\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[17], line 120\u001b[0m, in \u001b[0;36meval_with_interventions\u001b[0;34m(hf_model, nnsight_model, split_to_dataset, split_to_inv_locations, tokenizer, inv_dims, compute_metrics_fn, max_new_tokens, eval_batch_size, debug_print, forward_only, use_nnsight_replication, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     base_outputs \u001b[38;5;241m=\u001b[39m hf_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    112\u001b[0m         inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    113\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m     counterfactual_out_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnnsight_intervention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnnsight_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minv_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_to_inv_locations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_generated_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_reconstruction_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     eval_preds\u001b[38;5;241m.\u001b[39mappend(counterfactual_out_tokens)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m, in \u001b[0;36mnnsight_intervention\u001b[0;34m(nnsight_model, layer, autoencoder, inv_dims, inputs, split_to_inv_locations, n_generated_tokens, device, add_reconstruction_error, inv_positions, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m generated_pos_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(generated_inputs, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 41\u001b[0m     generated_pos_ids[batch_idx, \u001b[38;5;241m-\u001b[39mn_tokens[batch_idx]:] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_generated_tokens):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), nnsight_model\u001b[38;5;241m.\u001b[39mtrace(generated_inputs, attention_mask\u001b[38;5;241m=\u001b[39mgenerated_attn_mask, position_ids\u001b[38;5;241m=\u001b[39mgenerated_pos_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnnsight_tracer_kwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_nnsight_replication = True\n",
    "\n",
    "# Run eval\n",
    "import re\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test') or k.endswith('-val')\n",
    "                         }\n",
    "\n",
    "target_task = 'Country'\n",
    "max_new_tokens = 3\n",
    "print(f'Layer={layer_idx}')\n",
    "\n",
    "for inv_name, inv_dims in intervention_dim_to_eval:\n",
    "  if inv_name == 'reconstruction':\n",
    "    continue\n",
    "  print(f'Intervention_dims={inv_dims}')\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "    hf_model=hf_model,\n",
    "    nnsight_model=nnsight_model,\n",
    "    split_to_dataset=eval_split_to_dataset,\n",
    "    split_to_inv_locations=SPLIT_TO_INV_LOCATIONS,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    inv_dims=inv_dims,\n",
    "    compute_metrics_fn=compute_metrics_string_matching,\n",
    "    eval_batch_size=64, # batchsize=32 yields 21GB RAM usage with RTX A6000\n",
    "    debug_print=False,\n",
    "    use_nnsight_replication=use_nnsight_replication,\n",
    "    device=device,\n",
    "  )\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{autoencoder_run_name.split(\".pt\")[0]}_{inv_name}_{max_new_tokens}tok_{target_task}.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuMUHhoBohzd"
   },
   "source": [
    "# Distributed Alignment Search (DAS/MDAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onC3-T2h0ulg"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "from src.methods.distributed_alignment_search import LowRankRotatedSpaceIntervention\n",
    "from src.methods.differential_binary_masking import DifferentialBinaryMasking\n",
    "import pyvene as pv\n",
    "from tqdm import tqdm, trange\n",
    "from scripts.train_intervention import train_intervention\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from src.utils.dataset_utils import get_multitask_dataloader\n",
    "from src.utils.intervention_utils import train_intervention_step, eval_with_interventions, get_intervention_config, remove_all_forward_hooks, remove_invalid_token_id\n",
    "from src.utils.metric_utils import compute_metrics, compute_cross_entropy_loss\n",
    "\n",
    "\n",
    "def get_short_model_name(model):\n",
    "  name_match = re.search('(llama-2-\\d+b|tinyllama|pythia-[\\d.]+b)', model.name_or_path.lower())\n",
    "  if name_match:\n",
    "    return name_match.group(1)\n",
    "  else:\n",
    "    return model.name_or_path.lower().split('-')[0]\n",
    "\n",
    "\n",
    "def run_exp(config):\n",
    "  inv_tasks = '+'.join([''.join(re.findall(r'[A-Za-z]+', t)) for t, l in config['training_tasks'].items() if 'match_source' in l])\n",
    "  control_tasks = '+'.join([''.join(re.findall(r'[A-Za-z]+', t)) for t, l in config['training_tasks'].items() if 'match_base' in l])\n",
    "  task_compressed = ((inv_tasks + '_ex_' + control_tasks) if control_tasks else inv_tasks)\n",
    "  method_name = 'multitask_method' if len(config['training_tasks']) > 1 else 'baseline_method'\n",
    "  if config['intervenable_config']['intervenable_interventions_type'] == LowRankRotatedSpaceIntervention:\n",
    "    method_name = method_name.replace('method', 'daslora')\n",
    "  elif config['intervenable_config']['intervenable_interventions_type'] == DifferentialBinaryMasking:\n",
    "    if config['regularization_coefficient'] > 1e-6:\n",
    "      method_name = method_name.replace('method', 'mask_l1')\n",
    "    else:\n",
    "      method_name = method_name.replace('method', 'mask')\n",
    "  split_to_inv_locations = config['split_to_inv_locations']\n",
    "  input_len = list(split_to_inv_locations.values())[0]['max_input_length']\n",
    "  inv_pos = min([x['inv_position'][0] for x in split_to_inv_locations.values()])\n",
    "  inv_loc_name = 'len%d_pos%s' % (input_len, 'e' if inv_pos != input_len - 1 else 'f')\n",
    "  training_data_percentage = int(config['max_train_percentage'] * 100)\n",
    "  suffix = f\"_cause{config['cause_task_sample_size']}\"\n",
    "  if any([v == 'match_base' for t, v in config['training_tasks'].items()]):\n",
    "    suffix += f'_iso{config[\"iso_task_sample_size\"]}'\n",
    "  layer = config['intervenable_config']['intervenable_layer']\n",
    "  run_name = (f\"{get_short_model_name(model)}-layer{layer}\"\n",
    "              f\"-dim{config['intervention_dimension']}\"\n",
    "              f\"-{method_name}_{config['max_output_tokens']}tok_\"\n",
    "              f\"{task_compressed}_{inv_loc_name}_ep{config['training_epoch']}{suffix}\")\n",
    "  config['run_name_prefix'] = run_name.rsplit('_ep', 1)[0]\n",
    "  print(run_name)\n",
    "  intervenable, intervenable_config = train_intervention(config, model, tokenizer, split_to_dataset)\n",
    "  # Save model.\n",
    "  torch.save({k: v[0].rotate_layer.weight for k, v in intervenable.interventions.items()},\n",
    "             os.path.join(MODEL_DIR, f'{run_name}.pt'))\n",
    "  print('Model saved to %s' % os.path.join(MODEL_DIR, f'{run_name}.pt'))\n",
    "  # Eval.\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "      intervenable, eval_split_to_dataset, split_to_inv_locations, tokenizer,\n",
    "      compute_metrics_fn=compute_metrics,\n",
    "      max_new_tokens=config['max_output_tokens'],\n",
    "      eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  # Logging.\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{run_name}_evalall.json'), 'w'))\n",
    "  print('Saved to %s' % os.path.join(MODEL_DIR, f'{run_name}_evalall.json'))\n",
    "  remove_all_forward_hooks(intervenable)\n",
    "  return intervenable\n",
    "\n",
    "\n",
    "attrs = list(ALL_ATTR_TO_PROMPTS)\n",
    "target_attr = 'Country'\n",
    "\n",
    "# Train on disentangling Country attribute only.\n",
    "training_tasks_list = [\n",
    "  {t: 'match_source'} for t in attrs if t == target_attr\n",
    "] + [\n",
    "    {t: 'match_source' if t == target_t else 'match_base' for t in attrs}\n",
    "    for target_t in attrs if target_t == target_attr\n",
    "]\n",
    "\n",
    "# eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "#                          if k.endswith('-test') or k.endswith('-val')}\n",
    "print(len(training_tasks_list))\n",
    "print(training_tasks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DGCvKEQxYsU"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = model.eval()\n",
    "\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 128\n",
    "\n",
    "lr = 1e-4\n",
    "for inv_layer in [14]:\n",
    "  for inv_dim in [64]:\n",
    "    for training_tasks in training_tasks_list:\n",
    "      for cause_task_sample_size in [20000]:\n",
    "        config = {\n",
    "            'regularization_coefficient': 0,\n",
    "            'intervention_dimension': inv_dim,\n",
    "            'max_output_tokens': 3,\n",
    "            'intervenable_config': {\n",
    "              'intervenable_layer': inv_layer,\n",
    "              'intervenable_representation_type': 'block_output',\n",
    "              'intervenable_unit': 'pos',\n",
    "              'max_number_of_units': 1,\n",
    "              'intervenable_interventions_type': LowRankRotatedSpaceIntervention,\n",
    "            },\n",
    "            'training_tasks': training_tasks,\n",
    "            'training_epoch': 3,\n",
    "            'split_to_inv_locations': SPLIT_TO_INV_LOCATIONS,\n",
    "            'max_train_percentage': 1.0 if len(training_tasks) <= 3 else 1.0,\n",
    "            'init_lr': lr,\n",
    "            'cause_task_sample_size': cause_task_sample_size,\n",
    "            'iso_task_sample_size': 4000,\n",
    "            'training_batch_size': TRAINING_BATCH_SIZE,\n",
    "            'task_to_prompts': ALL_ATTR_TO_PROMPTS,\n",
    "            'log_dir': os.path.join(MODEL_DIR, 'logs'),\n",
    "        }\n",
    "        intervenable = run_exp(config)\n",
    "\n",
    "\n",
    "# Training each method will take about 3.5 hrs on the hosted T4 runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glatG86QlJ4q"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgTg14xMlJCO"
   },
   "outputs": [],
   "source": [
    "# # The training script above has already included the evaluation part.\n",
    "# # Below is a standalone evaluation script in case you want to rerun evaluation.\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# import pyvene as pv\n",
    "# from src.utils.intervention_utils import load_intervenable, load_intervenable_with_pca, eval_with_interventions\n",
    "# from src.utils.metric_utils import compute_metrics\n",
    "\n",
    "\n",
    "# model_paths = [\n",
    "#     'tinyllama-layer14-dim64-multitask_daslora_3tok_Country_ex_Continent+Latitude+Longitude+Language+Timezone_len48_pose_ep3_cause20000_iso4000.pt',\n",
    "#     'tinyllama-layer14-dim64-baseline_daslora_3tok_Country_len48_pose_ep3_cause20000.pt',\n",
    "#  ]\n",
    "\n",
    "# eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "#                          if k.endswith('-test')\n",
    "#                          }\n",
    "# RUN_TO_EVAL_METRICS = {}\n",
    "# for i, run_name in enumerate(model_paths):\n",
    "#   print(run_name)\n",
    "#   layer = int(re.search('layer(\\d+)[_\\-]', run_name).group(1))\n",
    "#   run_name, ext = run_name.rsplit('.', 1)\n",
    "#   if 'pca' in run_name:\n",
    "#     intervenable = load_intervenable_with_pca(model, run_name + '.' + ext)\n",
    "#   elif 'causal_abstraction' in run_name:\n",
    "#     # NOTE: This is not available\n",
    "#     intervenable = load_causal_abstraction_intervenable(model, run_name)\n",
    "#   else:\n",
    "#     intervenable = load_intervenable(model, os.path.join(MODEL_DIR, run_name + '.' + ext))\n",
    "#   split_to_eval_metrics = eval_with_interventions(\n",
    "#       intervenable, eval_split_to_dataset, SPLIT_TO_INV_LOCATIONS if layer < 24 else SPLIT_TO_INV_LOCATIONS_LAST_TOK,\n",
    "#       tokenizer, compute_metrics_fn=compute_metrics, max_new_tokens=3, debug_print=False)\n",
    "#   json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{run_name}_evalall.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-_gzLeA1Qc7"
   },
   "source": [
    "# Compare Methods with Disentangle Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxZSB2bT36ra"
   },
   "outputs": [],
   "source": [
    "# Compute disentangle scores.\n",
    "\n",
    "from src.utils.metric_utils import compute_disentangle_score, compute_disentangle_scores_possible_empties\n",
    "\n",
    "\n",
    "tinyllama_dimension_to_log_path = {\n",
    "    'SAE': {d: f'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k_dim{d}_3tok_Country.json'\n",
    "            # Update the following dimensions to match your own results.\n",
    "            # SAE might have different feature dimensions from run to run due to\n",
    "            # randomness in the feature selection algorithm.\n",
    "            for d in [71, 325, 399, 536, 8192]\n",
    "    },\n",
    "    # 'DAS': {d: f'tinyllama-layer14-dim{d}-baseline_daslora_3tok_Country_len48_pose_ep3_cause20000_evalall.json'\n",
    "    #         for d in [16, 64]},\n",
    "    # 'MDAS': {d: f'tinyllama-layer14-dim{d}-multitask_daslora_3tok_Country1_ex_Continent+Latitude+Longitude+Language+Timezone_len48_pose_ep3_cause20000_iso4000_evalall.json'\n",
    "    #          for d in [16, 64]\n",
    "    # },\n",
    "}\n",
    "# tinyllama_dimension_to_log_path['RandomSAE'] = {\n",
    "#             64: f'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k_random_dim64_3tok_Country.json',\n",
    "#             512: f'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k_random_dim512_3tok_Country.json'\n",
    "#             }\n",
    "\n",
    "entity_type = 'city'\n",
    "target_attribute = 'Country'\n",
    "split_type = 'context'\n",
    "split_suffix = '-test'\n",
    "model_name = 'tinyllama'\n",
    "\n",
    "\n",
    "split_to_raw_example = json.load(\n",
    "    open(os.path.join(DATA_DIR, model_name, f'{model_name}_{entity_type}_{split_type}_test.json')))\n",
    "attribute_to_prompts = json.load(\n",
    "    open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "\n",
    "attribute_to_iso_tasks = {\n",
    "    a: [p + split_suffix for p in ps if p + split_suffix in split_to_raw_example]\n",
    "    for a, ps in attribute_to_prompts.items() if a != target_attribute}\n",
    "attribute_to_cause_tasks = {\n",
    "    a: [p + split_suffix for p in ps if p + split_suffix in split_to_raw_example]\n",
    "    for a, ps in attribute_to_prompts.items() if a == target_attribute}\n",
    "\n",
    "print(attribute_to_iso_tasks)\n",
    "\n",
    "for key in attribute_to_iso_tasks:\n",
    "  print(key)\n",
    "\n",
    "for key in attribute_to_cause_tasks:\n",
    "  print(key, \"F\")\n",
    "\n",
    "\n",
    "method_to_data = collections.defaultdict(dict)\n",
    "for method in tinyllama_dimension_to_log_path:\n",
    "  for inv_dimension in tinyllama_dimension_to_log_path[method]:\n",
    "    log_data = json.load(\n",
    "        open(os.path.join(MODEL_DIR, tinyllama_dimension_to_log_path[method][inv_dimension])))\n",
    "\n",
    "    # print(log_data)\n",
    "\n",
    "    # for key in log_data:\n",
    "    #   print(key, log_data[key], \"\\n\")\n",
    "\n",
    "    method_to_data[method][inv_dimension] = compute_disentangle_scores_possible_empties(\n",
    "        log_data, attribute_to_iso_tasks, attribute_to_cause_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming method_to_data is defined elsewhere in your code\n",
    "# method_to_data = defaultdict(<class 'dict'>, {'SAE': {71: {'disentangle': 0.41708333333333336, 'isolate': 0.49666666666666665, 'cause': 0.3375}, 325: {'disentangle': 0.4729166666666667, 'isolate': 0.06333333333333334, 'cause': 0.8825000000000001}, 399: {'disentangle': 0.4741666666666667, 'isolate': 0.05333333333333334, 'cause': 0.895}, 536: {'disentangle': 0.4779166666666667, 'isolate': 0.06333333333333334, 'cause': 0.8925000000000001}, 8192: {'disentangle': 0.47833333333333333, 'isolate': 0.06666666666666667, 'cause': 0.89}}})\n",
    "\n",
    "# Extract data for SAE method\n",
    "sae_data = method_to_data['SAE']\n",
    "\n",
    "# Extract x and y values for each metric\n",
    "x = list(sae_data.keys())\n",
    "metrics = ['disentangle', 'isolate', 'cause']\n",
    "y_values = {metric: [sae_data[key][metric] for key in x] for metric in metrics}\n",
    "\n",
    "# Create a single plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each metric\n",
    "for metric, values in y_values.items():\n",
    "    plt.plot(x, values, marker='o', label=metric)\n",
    "\n",
    "# Set plot attributes\n",
    "plt.title('SAE Metrics')\n",
    "plt.xlabel('Key')\n",
    "plt.ylabel('Score')\n",
    "plt.xscale('log')  # Set x-axis to logarithmic scale\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0GSxcY1esIh"
   },
   "outputs": [],
   "source": [
    "# #@markdown Plotting\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 100\n",
    "# plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "\n",
    "# colors = [matplotlib.colors.to_hex(c) for c in plt.cm.tab20.colors]\n",
    "\n",
    "# name_to_color = {\n",
    "#     'SAE_RAND': 'gray',\n",
    "#     'PCA': colors[6],\n",
    "#     'SAE': colors[2],\n",
    "#     'RLAP': colors[4],\n",
    "#     'DBM': colors[1],\n",
    "#     'MDBM': colors[0],\n",
    "#     'DAS': colors[9],\n",
    "#     'MDAS': colors[8],\n",
    "# }\n",
    "\n",
    "# name_to_marker = {\n",
    "#     'SAE_RAND': 'o--',\n",
    "#     'PCA': 'o--',\n",
    "#     'SAE': 'o--',\n",
    "#     'RLAP': '^--',\n",
    "#     'DBM': 's--',\n",
    "#     'MDBM': 's--',\n",
    "#     'DAS': 's--',\n",
    "#     'MDAS': 's--',\n",
    "# }\n",
    "\n",
    "# for n, x in method_to_data.items():\n",
    "#   sorted_dim = sorted(x, key=lambda i: float(i[:-1]))\n",
    "#   p = plt.plot([x[k][2] for k in sorted_dim],\n",
    "#                [x[k][1] for k in sorted_dim], name_to_marker[n], label=n, markersize=10,\n",
    "#                c=name_to_color[n])\n",
    "#   for k in sorted(x, key=lambda s: x[s][0], reverse=True):\n",
    "#     c = p[-1].get_color()\n",
    "#     offset = (0, 0.05)\n",
    "#     # Shift text boxes to avoid overlaps.\n",
    "#     if n == 'SAE' and k == '3.8%':\n",
    "#       offset = (0.05, -0.07)\n",
    "#     plt.annotate(k, (x[k][2] - offset[0], x[k][1] + offset[1]), size=12,\n",
    "#                  bbox=dict(boxstyle='round,pad=0.15', fc=c, ec='white', alpha=0.5))\n",
    "# plt.scatter(1, 1, s=500, marker='*', color='gold', zorder=3)\n",
    "# plt.annotate('GOAL', (1.0-0.18, 1.0 - 0.01), size=12)\n",
    "# plt.gca().set_aspect('equal')\n",
    "# plt.xlim(-0.1, 1.05)\n",
    "# plt.ylim(-0.0, 1.1)\n",
    "# plt.grid(alpha=0.3, linestyle='--')\n",
    "# plt.legend(loc = 'lower left', prop={'size': 10})\n",
    "# plt.xlabel('Cause Score', fontsize=12)\n",
    "# _ = plt.ylabel('Isolate Score', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
